{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Djhay\\OneDrive\\Desktop\\Projects\\Hackathon\\Hackathon\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_markdown_files(root_dir, annoy_index, model):\n",
    "    \"\"\"\n",
    "    Crawls through directory and extracts text from markdown files.\n",
    "    Returns a dict with filepath as key and content as value.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    id_mapping = {}\n",
    "    id_counter = 0\n",
    "    for path in Path(root_dir).rglob('*.md'):\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                txt = f.read()\n",
    "                splits = split_text(txt) # -> []\n",
    "                splits = usefulness_filter(splits)\n",
    "                id_counter, id_mapping = encode_and_store(splits, path, model, annoy_index, id_counter, id_mapping)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {path}: {e}\")\n",
    "\n",
    "    return id_mapping\n",
    "\n",
    "def usefulness_filter(split_text):\n",
    "    # This function will probably become it's own module\n",
    "    # There are lots of potential ways we could filter before embedding\n",
    "    # and it might not be easy to determine what is best.\n",
    "    filtered = []\n",
    "\n",
    "    for chunk in split_text:\n",
    "        # Rule 1: Remove empty or whitespace-only chunks\n",
    "        if not chunk.strip():\n",
    "            continue\n",
    "        \n",
    "        # Rule 2: Exclude very short chunks (less than 5 characters)\n",
    "        if len(chunk) < 5:\n",
    "            continue\n",
    "\n",
    "        # Rule 3: Remove non-alphanumeric chunks\n",
    "        if not any(char.isalnum() for char in chunk):\n",
    "            continue\n",
    "\n",
    "        # Add to the filtered list if it passes all checks\n",
    "        filtered.append(chunk)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def split_text(text):\n",
    "    \"\"\"\n",
    "    Splits the input text into segments based on the following rules:\n",
    "    - Split on ., ?, !\n",
    "    - Split on new lines\n",
    "    \"\"\"\n",
    "    # Use a regular expression to split on ., ?, !, or new lines (\\n)\n",
    "    segments = re.split(r'[.?!\\n]+', text)\n",
    "    \n",
    "    # Remove any empty strings or leading/trailing whitespace\n",
    "    return [segment.strip() for segment in segments if segment.strip()]\n",
    "\n",
    "def encode_and_store(split_text, path, model, annoy_index, id_counter, id_mapping):\n",
    "    # Indexes: Path_id1, Path_id2\n",
    "    # Encode text and store in Annoy\n",
    "    for txt in split_text:\n",
    "        embedding = model.encode(txt)\n",
    "        id_mapping.update({id_counter: str(path)})\n",
    "        annoy_index.add_item(id_counter, embedding)\n",
    "        id_counter += 1\n",
    "    return id_counter, id_mapping\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from annoy import AnnoyIndex\n",
    "\n",
    "# Parameters\n",
    "embedding_dim = 384  # Dimensionality of your embeddings\n",
    "metric = 'angular'  # Metric for similarity ('angular', 'euclidean', 'manhattan', etc.)\n",
    "annoy_index = AnnoyIndex(embedding_dim, metric)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "root_dir = Path(\"TestData\\\\work\")\n",
    "id_mapping = crawl_markdown_files(root_dir, annoy_index, model)\n",
    "\n",
    "# Build the index (number of trees affects query speed/accuracy tradeoff)\n",
    "num_trees = 10\n",
    "annoy_index.build(num_trees)\n",
    "\n",
    "# Save the index to a file\n",
    "annoy_index.save('embeddings.ann')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_annoy(query, model, id_mapping):\n",
    "    query_embedding = model.encode(query)\n",
    "    results = annoy_index.get_nns_by_vector(query_embedding, 5, include_distances=True)\n",
    "    top_index = results[0][0]\n",
    "    top_doc = id_mapping[top_index]\n",
    "    return top_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestData\\work\\Now.md\n"
     ]
    }
   ],
   "source": [
    "print(query_annoy(\"Trickiest part of hedge effectiveness report\", model, id_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the index later:\n",
    "loaded_index = AnnoyIndex(embedding_dim, metric)\n",
    "loaded_index.load('embeddings.ann')\n",
    "\n",
    "# Perform a similarity search\n",
    "query_vector = [0.2, 0.3, 0.4, ..., 0.128]  # Replace with actual query embedding\n",
    "nearest_ids = loaded_index.get_nns_by_vector(query_vector, 5, include_distances=True)\n",
    "print(nearest_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_result = crawl_markdown_files('TestData\\\\work')\n",
    "list_crawled_result_values = list(crawled_result.values())\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# model = SentenceTransformer(\"msmarco-distilbert-dot-v5\")\n",
    "f = 768  # Length of item vector that will be indexed\n",
    "\n",
    "crawled_result_embeddings = model.encode(\n",
    "    list_crawled_result_values,\n",
    "    convert_to_tensor=True\n",
    ")\n",
    "crawled_result_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
